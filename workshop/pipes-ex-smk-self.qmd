---
title: Day 2 - Part 3 
format: 
  html: default
summary: intro to workflows
css: ./web.css
---

```{r, echo = FALSE, results='asis'}
library(webexercises)

knitr::opts_chunk$set(echo = FALSE)

# Uncomment to change widget colours:
style_widgets(incorrect = "red", correct = "green", highlight = "firebrick")

```

::: {.hello-exercise .alt-background}
:::{.content-block}

:::{.hello-exercise-banner}

<ul class="nav nav-pills" id="exercise-guide" role="tablist">
  <li class="nav-item" role="presentation">
    <button class="nav-link active" id="ucloud-tab" data-bs-toggle="tab" data-bs-target="#ucloud" type="button" role="tab" aria-controls="ucloud" aria-selected="true">UCloud</button>
  </li>
  <li class="nav-item" role="presentation">
    <button class="nav-link" id="locally-tab" data-bs-toggle="tab" data-bs-target="#locally" type="button" role="tab" aria-controls="locally" aria-selected="false">Local Setup</button>
  </li>
</ul>
:::


<div class="tab-content" id="hello-exercise-tabcontent">

<div class="tab-pane fade  show active" id="ucloud" role="tabpanel" aria-labelledby="ucloud-tab">

Follow these steps to run the exercises on UCloud. First, mount the following two drives, use the `setup.sh` initialization file and ask for 2 CPUs so we can run things in parallel: 

- `YourNameSurname#xxxx`: save your results/files here.  
- `hpclab-workshop`: this contains input files and scripts. You can read-only access from this directory (no write permissions). 

Next, activate `snakemake` environment. 

```{.bash}
conda deactivate 
# make sure no env is active!
conda activate snakemake 
```

Finally, navigate to your personal drive and use the project directory you created yesterday to save all the output files from the exercises!

</div>

<div class="tab-pane fade" id="locally" role="tabpanel" aria-labelledby="locally-tab">

You can choose to run things locally or on an HPC you have access to. 

</div>
</div>

:::
:::



It's time to create your own pipeline and experiment with its different components!

## A. Snakemake 

::: {.webex-check .callout-exercise}
# I - Build your pipeline from scratch 

You will be working and running the workflow from `hpclab` so make sure that is your working directory. Let's create one more subdir under `hpclab` named `scripts`.This is how your project structure should look like so far: 

```.bash
# folder structure  
mypersonaldrive/
│
├── envs/
│   └── [environment files here] 
│
└── hpclab/
    ├── results/
    │   ├── EAS.txt 
    │   ...
    ├── rules/
    │   ├── step1_wf.smk    <- created in next step 
    └── scripts 
```

Now, open a new file in your Visual Studio Code application under `rules` and name it `step1_wf.smk`. In this file, include the following two lines of code:

```{.bash}
#!/usr/bin/env python 
# -*- coding: utf-8 -*-
```

:::{.callout-hint}
# what are those lines for?
- `!/usr/bin/env python`: tells the system to use the python interpreter to execute the script
- `-*- coding: utf-8 -*-`: specifies the character encoding for the file as UTF-8
:::

![My Coder app](./images/smk_coder.png)

Let's define a very simple workflow of one task. Assuming `step1_wf.smk` is already open, follow these steps:

- Name of the rule: `preprocess`  
- **Do not use wildcards yet**
- Define the rule input: metadata file ("/work/HPCLab_workshop/data/samples_EB.tsv")
- Define the rule output: preprocessed file ("results_eb/samples_EB_cleaned.tsv"). Do not create `results_eb`, snakemake will do it for you.
- Use a shell command to run a Python script that removes samples with missing values (NA)

Your rule should replicate the behavior of the bash script shown below.

```{.bash filename="Terminal"}
INFILE="/work/HPCLab_workshop/data/samples_EB.tsv"
OUTPUT="/work/<nameSurname#xxx>/hpclab/results_eb/samples_EB_cleaned.tsv"

# This is what the pipeline's task is:
python scripts/preprocess.py $INFILE $OUTFILE
```

Create a new file and save it as `preprocess.py` inside the `scripts` folder of your project directory. Copy the following code into `preprocess.py`:

```{.python filename="preprocess.py"}
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import pandas as pd

def preprocess(input_file, output_file):
    df = pd.read_csv(input_file, sep='\t')  # Read the input TSV file
    df_cleaned = df.dropna()  # Remove rows with missing values
    df_cleaned.to_csv(output_file, sep='\t', index=False)  # Save the cleaned data to output file

if __name__ == "__main__":
    input_file = sys.argv[1]  # First argument: input file
    output_file = sys.argv[2]  # Second argument: output file
    preprocess(input_file, output_file)
```

:::{.callout-hint}
# Solution 
```{.bash filename="step1_wf.smk"}
rule preprocess:
  input:
    "/work/HPCLab_workshop/data/samples_EB.tsv"
  output:
    tsv="results/samples_EB_cleaned.tsv"
  shell:
    """
    python scripts/preprocess.py {input} {output}
    """
```
:::

All set! You're ready to run the workflow now. As a best practice, we recommend starting with a dry-run to ensure everything works correctly before executing the full workflow: `snakemake -s rules/step1_wf.smk -n`. **If there are no errors, go ahead and run it!**

:::{.callout-warning}
Since there is only one rule in this workflow, there is no need to define a rule all, as it will run automatically.
:::

What if we wanted to run this same rule for the dataset used in the previous Snakemake exercise? **How could we modify the rule or workflow to accommodate that dataset?**

You can leverage Snakemake's customization by introducing variables through either `params`, `wildcards`, or other mechanisms. By using these variables, you can dynamically adjust input files, tool settings, or even paths, making it easier to switch datasets or configurations without manually editing the rule each time.

#### Wildcards 

Wildcards are dynamic placeholders in rule definitions that allow for flexible input or output file names. You define them in your rules, and their actual values are determined when the workflow is executed, enabling versatile file handling across different workflow runs.

Modify the previous rule so that you can run the workflow for different datasets. Not sure how?

- Define a wildcard named `dataset` which represents the variable parts of your input or output (e.g. `EB` in this case)
- Dry-run, is it working  `snakemake -s rules/step1_wf.smk -np results/samples_EB_cleaned.tsv`

:::{.callout-hint}
# Solution 
```{.bash filename="step1_wf.smk"}

rule preprocess:
  input:
    "/work/HPCLab_workshop/data/samples_{dataset}.tsv"
  output:
    tsv="results/samples_{dataset}_cleaned.tsv"
  shell:
    """
    python scripts/preprocess.py {input} {output}
    """
```
:::

- Define variable datasets=["EB", "1kgp"]

:::{.callout-hint}
# Solution 
```{.bash filename="step1_wf.smk"}
samples = ["sample1", "sample2", "sample3"]

rule all:
    input:
        expand("results/{sample}/output.txt", sample=samples)

rule process_sample:
    input:
        "data/{sample}.txt"
    output:
        "results/{sample}/output.txt"
    params:
        parameter1=config.get("parameter1", 5),
        parameter2=config.get("parameter2", "abc")
    shell:
        "process_data.py {input} {params.parameter1} {params.parameter2} > {output}"
```
:::

#### Params 

#### Config files 

- config file for params and the path to dataset? 
- params software 
- log out // benchmark 

#### Resources 
- cores 
- resources related to size of the file 

#### Environment integration 
- conda env 
- create another smk with preprocessing to use include: 
- wildcards constrains? 

:::


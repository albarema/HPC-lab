---
title: Docker
format: 
  html
date-modified: last-modified
summary: workflow
---

:::{.callout-warning title="Requirements"}
Register on Docker Hub at docker.com and follow the [Docker Desktop guide](https://docs.docker.com/desktop/) to get started. We recommend going through [this tutorial](https://docker-curriculum.com/) before diving in. 
:::

Docker helps developers build, share, run, and verify applications anywhere — without tedious environment configuration or management. Before diving into hands-on activities, make sure these three main concepts are clear:

- **Image**: A docker image comprises a filesystem, default environment variables, a default command to execute, , and metadata about the image's creation and configuration.
- **Container**: A container is a running instance of a Docker image. It represents an isolated process that operates independently from other processes on the system, using the image as its environment.
- **Registry**: Registry: A registry is a repository for storing Docker images. Docker Hub is a public registry where many images can be found, but each local Docker daemon also maintains its own registry to manage and store images locally.

## Using Docker images 

First, we’ll explore some basic commands using existing Docker images available on Docker Hub. This will help you become familiar with Docker's functionality before we dive into creating our own custom images. People commonly use shorthand Docker commands like `docker pull` and `docker run`, though the full versions (e.g., `docker image pull`, `docker container run`) can clarify the underlying concepts and their specific functions.

**Docker Hub**

- Search for images `docker search <name>` (e.g. docker search debian)
- Download an image `docker pull <image-name>`

**Local Docker Daemon**

- Display all docker images currently stored on your local Docker daemon `docker images` (alias for `docker image ls`)
- Inspect docker image `docker inspect <image_name>` (alias for `docker image inspect`)
- Run a command (cmd) in a container `docker run <image_name> cmd` (alias for `docker container run  <image_name> cmd`)
- Start an interactive bash shell `docker run -it <image_name> bash`. Add other flags like:
  -  `-p 8888:8888` to access your interactive shell through 'localhost:8888' on your host. 
  - `-rm` to automatically delete the container once it stops, keeping your system clean (including its filesystem changes, logs and metadata). If you don't run this flag, a container will automatically be created and information about tje processes will be kept. Check all containers in your Docker daemon `docker container ls -a`
  - `--user=$(id -u):$(id -g)` useful if you are using sharing volumes and need appropriate permissions on the host to manipulate files. 
- Share the current directory with the container `docker run -it --volume=$(pwd):/directory/in/container image_name bash`, the output of pwd will be mounted to the /directory/in/container (e.g. data, shared, etc.)
- Manage your containers using pause or stop  
- Tag images with a new name `docker image tag image_name:tag new_name:tag`
- `docker logs <container_id>` 
- Remove images and clean up your hard drive `docker rmi <image_name>`
- Remove containers `docker container rm <container_name>`. Alternatively, remove all dead containers: `docker container prune`

All Docker containers have a **digest** which is the`sha256` hash of the image. It allows to uniquely identify a docker image and it is great for reproducibility.


:::{.callout-exercise title="Exercise init"}
We use a [Debian stable image](https://hub.docker.com/layers/library/debian/stable-20240812/images/sha256-540ebf19fb0bbc243e1314edac26b9fe7445e9c203357f27968711a45ea9f1d4?context=explore) (sha256-540ebf19fb0bbc243e1314edac26b9fe7445e9c203357f27968711a45ea9f1d4) as an example for pulling and inspecting because it provides a reliable, minimal base with essential tools. This image includes fundamental utilities like `bash` for shell scripting and `apt` for package management. It can be an ideal starting point for developing and testing pipelines or configuring server environments, offering a stable and consistent foundation that can be customized with additional packages as needed.

**1. Get a container image**

- Pull docker image (using tag stable otherwise, latest will be pulled by default) 
  ```{.bash .code-overflow-wrap} 
  docker pull debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912
  ```
**2. Run commands in a container**
- List the content of the container 
  ```{.bash .code-overflow-wrap} 
  docker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 ls
  ```
- Check is there is `python` or `perl` in this container: 
  ```{.bash .code-overflow-wrap}
  docker run -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 which -a python perl
  ```

**3. Use docker interactively** 

- Enter the container interactively with a Bash shell 
  ```{.bash .code-overflow-wrap}
  docker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 /bin/bash
  ```
- Now, collect info about the packages installed in the environment. 
  ```{.bash filename="Interactive Docker container"}
  hostname
  whoami
  ls -la ~/
  python  
  echo "Hello world!" > ~/myfile.txt
  ls -la ~/
  ```
**4. Exit and check the container** 
- Exit the container 
  ```{.bash} 
  exit
  ```

Now, rerun commands from step 3 under "Interactive Docker container". Does the output look any different?

:::{.callout-hint}
  You will notice that you are the root but the name of the machine has now changed and the file that you had created has disappeared.
:::

**5. Inspect the docker image** 

  ```{.bash .code-overflow-wrap}
  docker image inspect debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912
  ```
**6. Inspect Docker Image Details**
Identify the date of creation, the name of the field with the digest of the image and command run by default when entering this container.

**7. Remove container**

  ```{.bash .code-overflow-wrap}
  docker image docker rmi debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912
  ```
:::

When you exit a container, any changes made are lost because each new container starts in a clean state. Containers are isolated environments that don't retain memory of past interactions, allowing you to experiment without affecting your system. This immutability ensures that your program will run in the same conditions in the future, even months later. So, how can you retrieve the results of computations performed inside a container?

Importantly, when running the Docker container using the command for mounting, it is recommended to execute it from the **temporary directory** (e.g. `/tmp/` ) or a **project-specific** directory rather than your home directory. This way you would keep a clean and isolated environment within the container. 

#### Named volumes 
```{.bash}
# -v: mounting only your project-specific dir 
docker run -it --volumes <project_dir>
# Named volumes: are managed by Docker and are isolated from your host file system
docker volume create <my_project_volume>
docker run -it --volumes <my_project_volume>
```

Now, we're using named volumes or shared directories. When you make changes in these directories within the container, the updates are instantly reflected in the corresponding directory on the host machine. This allows seamless synchronization between the container and the host file system.

:::{.callout-warning title="Why not mounting your home directory"}
This is because the command mounts the current working directory (${PWD}) into the container at /home/rstudio. If you run this command from your home directory, any local packages or configurations (such as R or Python packages installed with install.packages or pip) would be accessible inside the container. This could compromise the isolation benefits of using the container, as it would inadvertently include your personal setup and packages.
:::

It is important to store container images in a shared storage area (e.g., with git or git annex).

### Transfer and backup Docker images 

Saving a Docker image as a tar file is useful because it enables you to transfer the image to another system or operating system without needing access to the original Docker registry. The tar file contains several key components: 

- metadata: JSON files essential to reconstruct the image 
  - **layer information** with each layer associated with metadata that includes which commands are used to create the later.
    ```{.bash filename="Dockerfile"} 
    FROM ubuntu:20.04 # Layer 1 - base image 
    RUN apt-get update && apt-get install -y python3 # Layer 2 - the result of running the command 
    COPY . /app # Layer 3 - add application files to the images
    ```
  - **tags** pointers to specific image digests or versions.
  - **history** of the image and instructions from the DOckerfile that were used to build the image.
  - **manifest** which ties together the layers and the overall image structure.
- Filesystem: the actual content, files and directories that make up the image. 

```{.bash}
# save to tar file 
docker image save --output=image.tar image_name 
# load tar file
docker image load --input=image.tar  
```

In some cases, you might only need to access the content of the filesystem of a Docker image (debugging, backup, repurposing. In such cases, you will need to create the container and export the root's filesystem to a tar file. Similarly, you can create a new image from the tar file. 

```{.bash}
docker container create --name=temp_container image_name
docker container export --output=image.tar temp_container
docker container rm temp_container
# new image importing tar file 
docker image import --input image.tar image_name
```

## Building Docker images

We’re now ready to build custom Docker images. We’ll start with a minimal pre-built image, add the necessary software, and then upload the final image to Docker Hub. 

A common practice is to start with a minimal or pre-configured image from Docker Hub. You then use a **Dockerfile** to modify this base image, specifying instructions for software installation and configuration. he Dockerfile acts as a recipe, providing a list of commands to build the image as needed.

```{.bash filename="Dockerfile"}
# deploy docker container
FROM node

# Info and rights of the app
LABEL software="App_name - sandbox" \
    maintainer="<author.address@sund.ku.dk>"

# root: needed to install and modify the container 
USER 0 

# run bash commands (eg. installing packages or softwares)
RUN mkdir -p

# 
RUN apt update \
    && apt install -y jupyter-notebook python3-matplotlib python3-pandas python3-numpy  \
    && rm -fr node_modules # removes dependencies

# set working directory (this directory should act as the main application directory)
WORKDIR /app

# copy files to-from directories
COPY /from/path/... /to/path/...

# set environment variables for Conda
ENV

# change up user permissions 
USER 11042
```

In this example, as indicated with the RUN  command, it will update the package list and install jupyter and everything else required. It is a common practice to remove dependencies with the command in the example above. 

:::{.callout-note title="Label" collpase="true"}
Key-value pair syntax, where should add at least these labels to the container:

- software = name of the app
- author = maintainer or author of the app
- version = version of the app
- license = app licence, e.g., "MIT"
- description = very short description of the app
:::

After preparing your Dockerfile, run `docker build` to create a new image based on these instructions. Docker’s isolation ensures that any changes made within a container are lost when you exit. Nevertheless, you can use `docker commit` to save these changes by creating a new image from the updated container. This new image can then be used to create containers with your modifications.

```{.bash}
docker build -t 
```

-p: port to use, usually 8787:8787 works for R studio
  - R studio: 8787:8787
  - JupyterLab: 8888:8888



<!--
```{.bash title="Singularity"}
MAC: export VM=sylabs/singularity-3.0-ubuntu-bionic64 && \
    vagrant init $VM && \
    vagrant up && \
    vagrant ssh

singularity pull docker://debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912

# creates a sif 
singularity shell debian@sha256_2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912.sif

gzip --version # same as docker
hostname # vagrant (macß) different
whoami # vagrant (docker is root)

Install 
singularity shell debian@sha256_2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912.sif

> apt-get install filter

Alternative commands: singularity exec docker://debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 bash -c "apt install filter 2>&1 || true "
```
:::{.callout-tip}
# The docker tag step is required as singularity has sometimes trouble handling the sha256: image description
:::

:::{.callout-exercise}
1. docker pull rocker/rstudio@sha256:74cd0a76855cb1065dd04293df990e7f8003db2e97c2f0cc740062d445d9fcf4
docker run -it rstudio@sha256:74cd0a76855cb1065dd04293df990e7f8003db2e97c2f0cc740062d445d9fcf4

2. docker rmi rocker/rstudio@sha256:74cd0a76855cb1065dd04293df990e7f8003db2e97c2f0cc740062d445d9fcf4
:::
-->



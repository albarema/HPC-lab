{"title":"Snakemake","markdown":{"yaml":{"title":"Snakemake","format":{"html":{"toc":false}},"date-modified":"last-modified","date-format":"long","summary":"intro to workflows"},"headingText":"Basics","containsRefs":false,"markdown":"\n\nIn this section, we will guide you through transitioning from bash scripts or notebooks to workflows. This approach is particularly useful as computations become more complex. \n\n[Snakemake](https://snakemake.readthedocs.io/en/stable/#) is a text-based workflow management tool that uses a Python-based language with domain-specific syntax. Originally designed for scientific workflows in bioinformatics, Snakemake shares operational principles with [`make`](https://www.gnu.org/software/make/), from which it was inspired. Importantly, workflows in Snakemake are structured around data stored in files, and the tool supports parallel computing, cloud storage, and can manage computational environments.\n\nThe workflow is decomposed into **rules** that define how to obtain output files from input files. Snakemake infers dependencies and determines the execution order automatically, offering significantly more flexibility than its predecessor, `make`.\n\n1. Semantics: define rules, each rule can use different programs (e.g. bash to run a shell command, python, R, GNU core utilities)\n\n```{.bash filename=\"Snakefile\"}\nrule dw_metadata:\n  input: path2/filename.tsv\n  output: data/filename.tsv\n  shell: \n    \"wget {input} > {output}\"\n\nrule split_superpop:\n  input: data/filename.tsv\n  output: data/{superpop}.tsv\n  shell: \n    \"python process_pops.py {input} {output}\"\n\nrule avg_gender:\n  input: data/{superpop}.tsv\n  output: data/{superpop}_{gender}.png\n  shell: \n    \"python statsPlot_gender.py {input} {output}\"\n\n```\n\nThe only mandatory flag in Snakemake is the number of cores (`-c`) or jobs (`-j`), which indicates the number of processes that can run in parallel. To run the workflow in Snakemake, you can either:\n\n1. Request Snakemake to generate a specific file (using the exact filename as defined in the rule's output) \n\n```{.bash .code-overflow-wrap}\n# Call the last output (of the pipeline or the one you want to generate)\nsnakemake -c2 data/EUR_female.png\n```\n\n2. Specify the name of a rule or task. You can do this when wildcards are not present\n\n```{.bash .code-overflow-wrap}\n# Call the last rule\nsnakemake -c1 dw_metadata\n```\n\n3. Alternatively, in very common practice, determine what you want to run inside a Snakefile \n\n```{.bash .code-overflow-wrap}\nrule all:\n  input:\n    expand(data/{superpop}_{gender}.png, superpop=[\"EUR\"], gender=[\"female\", \"male\"])\n```\n\n2. Scaling up - rules generalisation using wildcards (e.g.: from one to several datasets)\nYou can refer by index or by name \n\n3. Dependencies are determined top-down \n\nFor a given target, a rule that can be applied to create it, is determined (a job)\nFor the input files of the rule, go on recursively, \nIf no target is specified, snakemake, tries to apply the first rule\n\n4. Rule all: target rule that collects results\n\n#### Job execution\nA job is executed if and only if:\n- otuput file is target and does not exist\n- output file needed by another executed job and does not exist\n- input file newer than output file\n- input file will be updated by other job (eg. changes in rules)\n- execution is force ('--force-all')\n\nYou can plot the DAG (directed acyclic graph) of the jobs \n\n#### Useful command line interface\n\n```{.bash}\n# dry-run (-n), print shell commands (-p)\nsnakemake -n -p\n# Snakefile named different in another location \nsnakemake --snakefile path/to/file.smoker\n# dry-run (-n), print execution reason for each job\nsnakemake -n -r\n# Visualise DAG of jobs using Graphviz dot command\nsnakemake --dag | dot -Tsvg > dag.svg\n```\n\n#### Defining resources\n```{.bash}\nrule myrule:\n  resources: mem_mb= 100 #(100MB memory allocation)\n  threads: X\n  shell:\n    \"command {threads}\"\n```\n\nLet's say you defined our rule myrule needs 4 works, if we execute the workflow with 8 cores as follows:\n```{.bash}\nsnakemake --cores 8\n```\nThis means that 2 'myrule' jobs, will be executed in parallel.\n\nThe jobs are schedules to maximize parallelization, high priority jobs will be scheduled first, all while satisfying resource constrains. This means: \n\nIf we allocate 100MB for the execution of 'myrule' and we call snakemake as follows:\n\n```{.bash}\nsnakemake --resources mem_mb=100 --cores 8\n```\nOnly one 'myrule' job can be executed in parallel (you do not provide enough memory resources for 2). The memory resources is useful for jobs that are heavy memory demanding to avoid running out of memory. You will need to benchmark your pipeline to estimate how much memory and time your full workflow will take. We highly recommend doing so, get a subset of your dataset and give it a go! Log files will come very handy for the resource estimation. Of course, the execution of jobs is dependant on the free resources availability (eg. CPU cores). \n\n```{.bash}\nrule myrule:\n  log: \"logs/myrule.log\"\n  threads: X\n  shell:\n    \"command {threads}\"\n```\n\nLog files need to define the same wildcards as the output files, otherwise, you will get an error. \n\n#### Config files\nYou can also define values for wildcards or parameters in the config file (YAML or JSON). This is recommended when the pipeline might be used several times at different time points, to avoid unwanted modifications to the workflow. parameterization is key for such cases. \n\nIt can be loaded with: `configfile: \"path/to/config.yaml\"`\n\n:::{.callout-warning}\nThe given path is interpreted relative to the working directory, not relative to the location of the snakefile that contains the statement. \n:::\n\n#### Cluster execution\nWhen working from cluster systems you can execute the workflow using -qsub submission command\n\n```{.bash}\nsnakemake --cluster qsub \n```\n\n#### Additional advanced features\n- modularization\n- handling temporary and protected files: very important for intermediate files that filled up our memory and are not used in the long run and can be deleted once the final output is generated. This is automatically done by snakemake if you defined them in your pipeline\nHTML5 reports\n- rule parameters\n- tracking tool versions and code changes: will force rerunning older jobs when code and software are modified/updated. \n- data provenance information per file\n- python API for embedding snakemake in other tools\n\n#### Create an isolated environment to install dependencies\nBasic file structure\n```{.bash}\n| - config.yml\n| - requirements.txt (commonly also named environment.txt)\n| - rules/\n|   | - myrules.smk\n| - scripts/\n|   | - script1.py\n| - Snakefile\n```\nCreate conda environment, one per project!\n\n```{.bash}\n# create env\nconda create -n myworklow --file requirements.txt\n# activate environment\nsource activate myworkflow\n# then execute snakemake\n```\n\n### Best Practices recommended by Snakemake \n\n- Snakemake (v5.11+) includes a linter to check code quality, helping ensure best practices, readability, and reproducibility. Use the linter before publishing or seeking help. Run it with:\n```{.bash}\nsnakemake --lint\n```\n- Apply **snakefmt** to format workflows prior to publication. \n- Always include test data when publishing on GitHub and configure [GitHub Actions](https://github.com/features/actions) for continuos testing. Predefined actions for testing and linting ([here](https://github.com/snakemake/snakemake-github-action)), and formatting ([here](https://github.com/snakemake/snakefmt#github-actions)) are available. \n- To improve discoverability, follow Snakemake’s recommended [workflow structure](https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html#distribution-and-reproducibility). \n- Use [config files](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#snakefiles-standard-configuration) and, if needed, tabular configuration for metadata and experiment information.  \n- Use command-line arguments for runtime settings like threads, resources or output folders such as `--set-threads`, `--set-resources`, `--set-default-resources`, and `--directory`. This makes workflows more readable, scalable, and portable.\n- Keep filenames short and consistent, but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\n- separate Python code like helper functions from rules (e.g. in a workflow/rules/common.smk file). This approach helps non-experts understand the workflow without having to dig into unnecessary internal details.\n- Avoid lambda expressions inside of rules.\n- Where possible, use [Snakemake wrappers](https://snakemake-wrappers.readthedocs.io/en/stable/) to simplify recurring tasks.\n\n\n## Sources\n\n- [Snakemake tutorial](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html#tutorial)\n- [Snakemake turorial slides by Johannes Koster](https://slides.com/johanneskoester/snakemake-tutorial)\n- [faircookbook worflows](https://faircookbook.elixir-europe.org/content/recipes/applied-examples/fair-workflows.html)\n- Formatter [snakefmt](https://github.com/snakemake/snakefmt)\n- [Snakemake workflow catalog](https://snakemake.github.io/snakemake-workflow-catalog/)\n- [Rules for inclusion in Snakemake workflow catalog](https://snakemake.github.io/snakemake-workflow-catalog/?rules=true)\n\nRecommended reading: \n\n- Köster, Johannes and Rahmann, Sven. \"Snakemake - A scalable bioinformatics workflow engine\". Bioinformatics 2012.\n- Köster, Johannes. \"Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis\", PhD thesis, TU Dortmund 2014.","srcMarkdownNoYaml":"\n\nIn this section, we will guide you through transitioning from bash scripts or notebooks to workflows. This approach is particularly useful as computations become more complex. \n\n#### Basics\n[Snakemake](https://snakemake.readthedocs.io/en/stable/#) is a text-based workflow management tool that uses a Python-based language with domain-specific syntax. Originally designed for scientific workflows in bioinformatics, Snakemake shares operational principles with [`make`](https://www.gnu.org/software/make/), from which it was inspired. Importantly, workflows in Snakemake are structured around data stored in files, and the tool supports parallel computing, cloud storage, and can manage computational environments.\n\nThe workflow is decomposed into **rules** that define how to obtain output files from input files. Snakemake infers dependencies and determines the execution order automatically, offering significantly more flexibility than its predecessor, `make`.\n\n1. Semantics: define rules, each rule can use different programs (e.g. bash to run a shell command, python, R, GNU core utilities)\n\n```{.bash filename=\"Snakefile\"}\nrule dw_metadata:\n  input: path2/filename.tsv\n  output: data/filename.tsv\n  shell: \n    \"wget {input} > {output}\"\n\nrule split_superpop:\n  input: data/filename.tsv\n  output: data/{superpop}.tsv\n  shell: \n    \"python process_pops.py {input} {output}\"\n\nrule avg_gender:\n  input: data/{superpop}.tsv\n  output: data/{superpop}_{gender}.png\n  shell: \n    \"python statsPlot_gender.py {input} {output}\"\n\n```\n\nThe only mandatory flag in Snakemake is the number of cores (`-c`) or jobs (`-j`), which indicates the number of processes that can run in parallel. To run the workflow in Snakemake, you can either:\n\n1. Request Snakemake to generate a specific file (using the exact filename as defined in the rule's output) \n\n```{.bash .code-overflow-wrap}\n# Call the last output (of the pipeline or the one you want to generate)\nsnakemake -c2 data/EUR_female.png\n```\n\n2. Specify the name of a rule or task. You can do this when wildcards are not present\n\n```{.bash .code-overflow-wrap}\n# Call the last rule\nsnakemake -c1 dw_metadata\n```\n\n3. Alternatively, in very common practice, determine what you want to run inside a Snakefile \n\n```{.bash .code-overflow-wrap}\nrule all:\n  input:\n    expand(data/{superpop}_{gender}.png, superpop=[\"EUR\"], gender=[\"female\", \"male\"])\n```\n\n2. Scaling up - rules generalisation using wildcards (e.g.: from one to several datasets)\nYou can refer by index or by name \n\n3. Dependencies are determined top-down \n\nFor a given target, a rule that can be applied to create it, is determined (a job)\nFor the input files of the rule, go on recursively, \nIf no target is specified, snakemake, tries to apply the first rule\n\n4. Rule all: target rule that collects results\n\n#### Job execution\nA job is executed if and only if:\n- otuput file is target and does not exist\n- output file needed by another executed job and does not exist\n- input file newer than output file\n- input file will be updated by other job (eg. changes in rules)\n- execution is force ('--force-all')\n\nYou can plot the DAG (directed acyclic graph) of the jobs \n\n#### Useful command line interface\n\n```{.bash}\n# dry-run (-n), print shell commands (-p)\nsnakemake -n -p\n# Snakefile named different in another location \nsnakemake --snakefile path/to/file.smoker\n# dry-run (-n), print execution reason for each job\nsnakemake -n -r\n# Visualise DAG of jobs using Graphviz dot command\nsnakemake --dag | dot -Tsvg > dag.svg\n```\n\n#### Defining resources\n```{.bash}\nrule myrule:\n  resources: mem_mb= 100 #(100MB memory allocation)\n  threads: X\n  shell:\n    \"command {threads}\"\n```\n\nLet's say you defined our rule myrule needs 4 works, if we execute the workflow with 8 cores as follows:\n```{.bash}\nsnakemake --cores 8\n```\nThis means that 2 'myrule' jobs, will be executed in parallel.\n\nThe jobs are schedules to maximize parallelization, high priority jobs will be scheduled first, all while satisfying resource constrains. This means: \n\nIf we allocate 100MB for the execution of 'myrule' and we call snakemake as follows:\n\n```{.bash}\nsnakemake --resources mem_mb=100 --cores 8\n```\nOnly one 'myrule' job can be executed in parallel (you do not provide enough memory resources for 2). The memory resources is useful for jobs that are heavy memory demanding to avoid running out of memory. You will need to benchmark your pipeline to estimate how much memory and time your full workflow will take. We highly recommend doing so, get a subset of your dataset and give it a go! Log files will come very handy for the resource estimation. Of course, the execution of jobs is dependant on the free resources availability (eg. CPU cores). \n\n```{.bash}\nrule myrule:\n  log: \"logs/myrule.log\"\n  threads: X\n  shell:\n    \"command {threads}\"\n```\n\nLog files need to define the same wildcards as the output files, otherwise, you will get an error. \n\n#### Config files\nYou can also define values for wildcards or parameters in the config file (YAML or JSON). This is recommended when the pipeline might be used several times at different time points, to avoid unwanted modifications to the workflow. parameterization is key for such cases. \n\nIt can be loaded with: `configfile: \"path/to/config.yaml\"`\n\n:::{.callout-warning}\nThe given path is interpreted relative to the working directory, not relative to the location of the snakefile that contains the statement. \n:::\n\n#### Cluster execution\nWhen working from cluster systems you can execute the workflow using -qsub submission command\n\n```{.bash}\nsnakemake --cluster qsub \n```\n\n#### Additional advanced features\n- modularization\n- handling temporary and protected files: very important for intermediate files that filled up our memory and are not used in the long run and can be deleted once the final output is generated. This is automatically done by snakemake if you defined them in your pipeline\nHTML5 reports\n- rule parameters\n- tracking tool versions and code changes: will force rerunning older jobs when code and software are modified/updated. \n- data provenance information per file\n- python API for embedding snakemake in other tools\n\n#### Create an isolated environment to install dependencies\nBasic file structure\n```{.bash}\n| - config.yml\n| - requirements.txt (commonly also named environment.txt)\n| - rules/\n|   | - myrules.smk\n| - scripts/\n|   | - script1.py\n| - Snakefile\n```\nCreate conda environment, one per project!\n\n```{.bash}\n# create env\nconda create -n myworklow --file requirements.txt\n# activate environment\nsource activate myworkflow\n# then execute snakemake\n```\n\n### Best Practices recommended by Snakemake \n\n- Snakemake (v5.11+) includes a linter to check code quality, helping ensure best practices, readability, and reproducibility. Use the linter before publishing or seeking help. Run it with:\n```{.bash}\nsnakemake --lint\n```\n- Apply **snakefmt** to format workflows prior to publication. \n- Always include test data when publishing on GitHub and configure [GitHub Actions](https://github.com/features/actions) for continuos testing. Predefined actions for testing and linting ([here](https://github.com/snakemake/snakemake-github-action)), and formatting ([here](https://github.com/snakemake/snakefmt#github-actions)) are available. \n- To improve discoverability, follow Snakemake’s recommended [workflow structure](https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html#distribution-and-reproducibility). \n- Use [config files](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#snakefiles-standard-configuration) and, if needed, tabular configuration for metadata and experiment information.  \n- Use command-line arguments for runtime settings like threads, resources or output folders such as `--set-threads`, `--set-resources`, `--set-default-resources`, and `--directory`. This makes workflows more readable, scalable, and portable.\n- Keep filenames short and consistent, but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\n- separate Python code like helper functions from rules (e.g. in a workflow/rules/common.smk file). This approach helps non-experts understand the workflow without having to dig into unnecessary internal details.\n- Avoid lambda expressions inside of rules.\n- Where possible, use [Snakemake wrappers](https://snakemake-wrappers.readthedocs.io/en/stable/) to simplify recurring tasks.\n\n\n## Sources\n\n- [Snakemake tutorial](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html#tutorial)\n- [Snakemake turorial slides by Johannes Koster](https://slides.com/johanneskoester/snakemake-tutorial)\n- [faircookbook worflows](https://faircookbook.elixir-europe.org/content/recipes/applied-examples/fair-workflows.html)\n- Formatter [snakefmt](https://github.com/snakemake/snakefmt)\n- [Snakemake workflow catalog](https://snakemake.github.io/snakemake-workflow-catalog/)\n- [Rules for inclusion in Snakemake workflow catalog](https://snakemake.github.io/snakemake-workflow-catalog/?rules=true)\n\nRecommended reading: \n\n- Köster, Johannes and Rahmann, Sven. \"Snakemake - A scalable bioinformatics workflow engine\". Bioinformatics 2012.\n- Köster, Johannes. \"Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis\", PhD thesis, TU Dortmund 2014."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../css/custom-callout.lua"],"toc":false,"css":["../include/webex.css"],"include-after-body":["../include/webex.js"],"output-file":"smk.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.550","copyright":"CC-BY-SA 4.0 license","theme":{"light":["materia","../css/materialight.scss"],"dark":"darkly"},"title":"Snakemake","date-modified":"last-modified","date-format":"long","summary":"intro to workflows"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}